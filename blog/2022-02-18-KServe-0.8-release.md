---
title: Announcing KServe v0.8
description: KServe 0.8 Release Blog Post
slug: kserve-0.8-release
authors:
- yuzisun
- pvaneck
- js-ts
- andyi2it

tags: [releases]
hide_table_of_contents: false
---

# Announcing: KServe v0.8

*Published on February 18, 2022*

Today, we are pleased to announce the v0.8.0 release of KServe! While the last release was focused on the [transition](https://blog.kubeflow.org/release/official/2021/09/27/kfserving-transition.html) of KFServing to KServe, this release was focused on unifying the InferenceService API for deploying models on KServe and ModelMesh.

>**Note**: For current users of KFServing/KServe, please take a few minutes to answer this [short survey](https://groups.google.com/g/kubeflow-discuss/c/B0trz3qZiJE) and provide your feedback!

<!-- truncate -->

## ‚ö†Ô∏è What's Changed

- **ONNX Runtime Server** has been removed from the supported serving runtime list. KServe by default now uses the **Triton Inference Server** to serve ONNX models.
- KServe's **PyTorchServer** has been removed from the supported serving runtime list. KServe by default now uses **TorchServe** to serve PyTorch models.
- A few main KServe SDK class names have been changed:
    - **KFModel** is renamed to **Model**
    - **KFServer** is renamed to **ModelServer**
    - **KFModelRepository** is renamed to **ModelRepository**

## üöÄ What's New

Some notable updates are:

- **ClusterServingRuntime** and **ServingRuntime** CRDs are introduced. Learn more [below](#-servingruntimes-and-clusterservingruntimes).
- A new **Model Spec** was introduced to the InferenceService Predictor Spec as a new way to specify models. Learn more [below](#-updated-inferenceservice-predictor-spec).
- **Knative 1.0** is now supported and certified for the KServe Serverless installation.
- **gRPC** is now supported for transformer to predictor network communication.
- **TorchServe** Serving runtime has been updated to 0.5.2 which now supports the KServe V2 REST protocol.
- **ModelMesh** now has multi-namespace support, and users can now deploy GCS or HTTP(S) hosted models.

## üîß ServingRuntimes and ClusterServingRuntimes

This release introduces two new CRDs *ServingRuntimes* and *ClusterServingRuntimes* with the only difference between these two is that one is namespace-scoped and one is cluster-scoped. A ServingRuntime defines the templates for Pods that can serve one or more particular model formats. Each ServingRuntime defines key information such as the container image of the runtime and a list of the model formats that the runtime supports.

In previous versions of KServe, supported predictor formats and container images were defined in a [config map](https://github.com/kserve/kserve/blob/release-0.7/config/configmap/inferenceservice.yaml#L7) in the control plane namespace. The ServingRuntime CRD should allow for improved flexibility and extensibility for defining or customizing runtimes to how you see fit without having to modify any controller code or any resources in the controller namespace.

Several out-of-the-box ClusterServingRuntimes are provided with KServe so that users can continue to use KServe how they did before without having to define the runtimes themselves.

**Example SKLearn ClusterServingRuntime:**

```yaml
apiVersion: serving.kserve.io/v1alpha1
kind: ClusterServingRuntime
metadata:
  name: kserve-sklearnserver
spec:
  supportedModelFormats:
    - name: sklearn
      version: "1"
      autoSelect: true
  containers:
    - name: kserve-container
      image: kserve/sklearnserver:latest
      args:
        - --model_name={{.Name}}
        - --model_dir=/mnt/models
        - --http_port=8080
      resources:
        requests:
          cpu: "1"
          memory: 2Gi
        limits:
          cpu: "1"
          memory: 2Gi
```

## üìã Updated InferenceService Predictor Spec

A new Model spec was also introduced as a part of the Predictor spec for InferenceServices. One of the problems KServe was having was that the InferenceService CRD was becoming unwieldy with each model serving runtime being an object in the Predictor spec. This generated a lot of field duplication in the schema, bloating the overall size of the CRD. If a user wanted to introduce a new model serving framework for KServe to support, the CRD would have to be modified, and subsequently the controller code.

Now, with the Model spec, a user can specify a model format and optionally a corresponding version. The KServe control plane will automatically select and use the *ClusterServingRuntime* or *ServingRuntime* that supports the given format. Each *ServingRuntime* maintains a list of supported model formats and versions. If a format has `autoselect` as `true`, then that opens the *ServingRuntime* up for automatic model placement for that model format.

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

<Tabs>
<TabItem value="new" label="New Schema">

```yaml
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: example-sklearn-isvc
spec:
  predictor:
    model:
      modelFormat:
        name: sklearn
      storageUri: s3://bucket/sklearn/mnist.joblib
```

</TabItem>
<TabItem value="previous" label="Previous Schema">

```yaml
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: example-sklearn-isvc
spec:
  predictor:
    sklearn:
      storageUri: s3://bucket/sklearn/mnist.joblib
```

</TabItem>
</Tabs>

The previous way of defining predictors is still supported, however, the new approach will be the preferred one going forward. Eventually, the previous schema, with the framework names as keys in the predictor spec, will be removed.

## üåê ModelMesh Updates

[ModelMesh](https://developer.ibm.com/blogs/kserve-and-watson-modelmesh-extreme-scale-model-inferencing-for-trusted-ai/) has been in the process of integrating as KServe's multi-model serving backend. With the inclusion of the aforementioned ServingRuntime CRDs and the Predictor Model spec, the two projects are now much more aligned, with continual improvements underway.

ModelMesh now supports multi-namespace reconciliation. Previously, the ModelMesh controller would only reconcile against resources deployed in the same namespace as the controller. Now, by default, ModelMesh will be able to handle InferenceService deployments in any "modelmesh-enabled" namespace. Learn more [here](https://github.com/kserve/modelmesh-serving/blob/release-0.8/docs/install/install-script.md#setup-additional-namespaces).

Also, while ModelMesh previously only supported S3-based storage, we are happy to share that ModelMesh now works with models hosted using GCS and HTTP(S).

## üîç Release Notes

To see all release updates, check out the KServe [release notes](https://github.com/kserve/kserve/releases/tag/v0.8.0) and ModelMesh Serving [release notes](https://github.com/kserve/modelmesh-serving/releases/tag/v0.8.0)!

## üôè Acknowledgments

We want to thank all the contributors who made this release possible:
- **Authors**: Dan Sun, Paul Van Eck, Vedant Padwal, Andrews Arokiam on behalf of the KServe Working Group
- **Core Contributors**: The KServe maintainers and working group members
- **Community**: Everyone who reported issues, provided feedback, and tested features

## ü§ù Join the Community

- Visit our [Website](https://kserve.github.io/website/) or [GitHub](https://github.com/kserve)
- Join the Slack ([#kubeflow-kfserving](https://kubeflow.slack.com/join/shared_invite/zt-n73pfj05-l206djXlXk5qdQKs4o1Zkg#/))
- Attend a [biweekly community meeting on Wednesday 9am PST](https://docs.google.com/document/d/1KZUURwr9MnHXqHA08TFbfVbM8EAJSJjmaMhnvstvi-k/edit#heading=h.4i9fb8ndp9vp)
- View our [developer](https://github.com/kserve/website/blob/main/docs/developer/developer.md) and [doc](https://github.com/kserve/website/blob/main/docs/help/contributor/mkdocs-contributor-guide.md) contribution guides to learn how to make contributions. We are excited to work with you to make KServe better and promote its adoption!

**Happy serving!**

---

*The KServe team is committed to making machine learning model serving simple, scalable, and standardized. Thank you for being part of our community!*
