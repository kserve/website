---
title: Announcing KServe v0.10.0
description: KServe 0.10 Release Blog Post
slug: kserve-0.10-release
authors: [yuzisun]
tags: [releases]
hide_table_of_contents: false
---

# Announcing: KServe v0.10.0

*Published on February 5, 2023*

We are excited to announce KServe 0.10 release. In this release we have enabled more KServe networking options, improved KServe telemetry for supported serving runtimes and increased support coverage for [Open(aka v2) inference protocol](https://kserve.github.io/website/0.10/modelserving/data_plane/v2_protocol/) for both standard and ModelMesh InferenceService.

<!-- truncate -->

## üåê KServe Networking Options

Istio is now optional for both [Serverless](https://kserve.github.io/website/0.10/admin/serverless/serverless/) and [RawDeployment](https://kserve.github.io/website/0.10/admin/kubernetes_deployment/) mode. Please see the [alternative networking guide](https://kserve.github.io/website/0.10/admin/serverless/kourier_networking/) for how you can enable other ingress options supported by Knative with Serverless mode.
For Istio users, if you want to turn on full service mesh mode to secure InferenceService with mutual TLS and enable the traffic policies, please read the [service mesh setup guideline](https://kserve.github.io/website/0.10/admin/serverless/servicemesh/).

## üìä KServe Telemetry for Serving Runtimes

We have instrumented additional latency metrics in KServe Python ServingRuntimes for `preprocess`, `predict` and `postprocess` handlers.
In Serverless mode we have extended Knative `queue-proxy` to enable metrics aggregation for both metrics exposed in `queue-proxy` and `kserve-container` from each `ServingRuntime`.
Please read the [prometheus metrics setup guideline](https://kserve.github.io/website/0.10/modelserving/observability/prometheus_metrics/) for how to enable the metrics scraping and aggregations.

## üöÄ Open(v2) Inference Protocol Support Coverage

As there have been increasing adoptions for `KServe v2 Inference Protocol` from [AMD Inference ServingRuntime](https://kserve.github.io/website/0.10/modelserving/v1beta1/amd/) which
supports FPGAs and OpenVINO which now provides KServe [REST](https://docs.openvino.ai/latest/ovms_docs_rest_api_kfs.html) and [gRPC](https://docs.openvino.ai/latest/ovms_docs_grpc_api_kfs.html) compatible API,
in [the issue](https://github.com/kserve/kserve/issues/2663) we have proposed to rename to `KServe Open Inference Protocol`.

In KServe 0.10, we have added Open(v2) inference protocol support for KServe custom runtimes.
Now, you can enable v2 REST/gRPC for both custom transformer and predictor with images built by implementing KServe Python SDK API.
gRPC enables high performance inference data plane as it is built on top of HTTP/2 and binary data transportation which is more efficient to send over the wire compared to REST.
Please see the detailed example for [transformer](https://kserve.github.io/website/0.10/modelserving/v1beta1/transformer/torchserve_image_transformer/) and 
[predictor](https://kserve.github.io/website/0.10/modelserving/v1beta1/custom/custom_model/).

```python
from kserve import Model

def image_transform(byte_array):
    image_processing = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    image = Image.open(io.BytesIO(byte_array))
    tensor = image_processing(image).numpy()
    return tensor

class CustomModel(Model):
    def predict(self, request: InferRequest, headers: Dict[str, str]) -> InferResponse:
        input_tensors = [image_transform(instance) for instance in request.inputs[0].data]
        input_tensors = np.asarray(input_tensors)
        output = self.model(input_tensors)
        torch.nn.functional.softmax(output, dim=1)
        values, top_5 = torch.topk(output, 5)
        result = values.flatten().tolist()
        response_id = generate_uuid()
        infer_output = InferOutput(name="output-0", shape=list(values.shape), datatype="FP32", data=result)
        infer_response = InferResponse(model_name=self.name, infer_outputs=[infer_output], response_id=response_id)
        return infer_response

class CustomTransformer(Model):
    def preprocess(self, request: InferRequest, headers: Dict[str, str]) -> InferRequest:
        input_tensors = [image_transform(instance) for instance in request.inputs[0].data]
        input_tensors = np.asarray(input_tensors)
        infer_inputs = [InferInput(name="INPUT__0", datatype='FP32', shape=list(input_tensors.shape),
                                   data=input_tensors)]
        infer_request = InferRequest(model_name=self.model_name, infer_inputs=infer_inputs)
        return infer_request
```

You can use the same Python API type `InferRequest` and `InferResponse` for both REST and gRPC protocol. KServe handles the underlying decoding and encoding according to the protocol.

‚ö†Ô∏è **Warning**: A new `headers` argument is added to the custom handlers to pass http/gRPC headers or other metadata. You can also use this as context dict to pass data between handlers.
If you have existing custom transformer or predictor, the `headers` argument is now required to add to the `preprocess`, `predict` and `postprocess` handlers.

Please check the following matrix for supported ModelFormats and [ServingRuntimes](https://kserve.github.io/website/0.10/modelserving/v1beta1/serving_runtime/).

| Model Format | v1           | Open(v2) REST/gRPC |
|--------------|--------------|--------------------|
| Tensorflow   | ‚úÖ TFServing  | ‚úÖ Triton           |
| PyTorch      | ‚úÖ TorchServe | ‚úÖ TorchServe       |
| TorchScript  | ‚úÖ TorchServe | ‚úÖ Triton           |
| ONNX         | ‚ùå            | ‚úÖ Triton           |
| Scikit-learn | ‚úÖ KServe     | ‚úÖ MLServer         |
| XGBoost      | ‚úÖ KServe     | ‚úÖ MLServer         |
| LightGBM     | ‚úÖ KServe     | ‚úÖ MLServer         |
| MLFlow       | ‚ùå            | ‚úÖ MLServer         |
| Custom       | ‚úÖ KServe     | ‚úÖ KServe           |

## üèóÔ∏è Multi-Arch Image Support

KServe control plane images [kserve-controller](https://hub.docker.com/r/kserve/kserve-controller/tags),
[kserve/agent](https://hub.docker.com/r/kserve/agent/tags), [kserve/router](https://hub.docker.com/r/kserve/router/tags) are now supported 
for multiple architectures: `ppc64le`, `arm64`, `amd64`, `s390x`.

## üîê KServe Storage Credentials Support

- Currently, AWS users need to create a secret with long term/static IAM credentials for downloading models stored in S3.
  Security best practice is to use [IAM role for service account(IRSA)](https://aws.amazon.com/blogs/opensource/introducing-fine-grained-iam-roles-service-accounts/) 
  which enables automatic credential rotation and fine-grained access control, see how to [setup IRSA](https://kserve.github.io/website/0.10/modelserving/storage/s3/s3/#create-service-account-with-iam-role).
- Support Azure Blobs with [managed identity](https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/how-manage-user-assigned-managed-identities?pivots=identity-mi-methods-azcli).

## üìä ModelMesh Updates

ModelMesh has continued to integrate itself as KServe's multi-model serving backend, introducing improvements and features that better align the two projects. For example, it now supports ClusterServingRuntimes, allowing use of cluster-scoped ServingRuntimes, originally introduced in KServe 0.8.

Additionally, ModelMesh introduced support for TorchServe enabling users to serve arbitrary PyTorch models (e.g. eager-mode) in the context of distributed-multi-model serving.

Other limitations have been addressed as well, such as adding support for BYTES/string type tensors when using the REST inference API for inference requests that require them.

## üîç Release Notes

For complete release notes including all changes, bug fixes, and known issues, visit the [GitHub release pages](https://github.com/kserve/kserve/releases/tag/v0.10.0) for KServe v0.10 and [ModelMesh v0.10](https://github.com/kserve/modelmesh-serving/releases/tag/v0.10.0).

## üôè Acknowledgments

We want to thank all the contributors who made this release possible:

**Individual Contributors:**
- [Steve Larkin](https://github.com/sel)
- [Stephan Schielke](https://github.com/stephanschielke)
- [Curtis Maddalozzo](https://github.com/cmaddalozzo)
- [Zhongcheng Lao](https://github.com/laozc)
- [Dimitris Aragiorgis](https://github.com/dimara)
- [Pan Li](https://github.com/panli889)
- [tjandy98](https://github.com/tjandy98)
- [Sukumar Gaonkar](https://github.com/sukumargaonkar)
- [Rachit Chauhan](https://github.com/rachitchauhan43)
- [Rafael Vasquez](https://github.com/rafvasq)
- [Tim Kleinloog](https://github.com/TimKleinloog)
- [Christian Kadner](https://github.com/ckadner)
- [ddelange](https://github.com/ddelange)
- [Lize Cai](https://github.com/lizzzcai)
- [sangjune.park](https://github.com/park12sj)
- [Suresh Nakkeran](https://github.com/Suresh-Nakkeran)
- [Konstantinos Messis](https://github.com/MessKon)
- [Matt Rose](https://github.com/matty-rose)
- [Alexa Griffith](https://github.com/alexagriffith)
- [Jagadeesh J](https://github.com/jagadeeshi2i)
- [Alex Lembiyeuski](https://github.com/alembiewski)
- [Yuki Iwai](https://github.com/tenzen-y)
- [Andrews Arokiam](https://github.com/andyi2it)
- [Xin Fu](https://github.com/xfu83)
- [adilhusain-s](https://github.com/adilhusain-s)
- [Pranav Pandit](https://github.com/pranavpandit1)
- [C1berwiz](https://github.com/C1berwiz)
- [dilverse](https://github.com/dilverse)
- [Yuan Tang](https://github.com/terrytangyuan)
- [Dan Sun](https://github.com/yuzisun)
- [Nick Hill](https://github.com/njhill)

**Core Contributors**: The KServe maintainers and working group members

**Community**: Everyone who reported issues, provided feedback, and tested features

## ü§ù Join the Community

- Visit our [Website](https://kserve.github.io/website/) or [GitHub](https://github.com/kserve)
- Join the Slack ([#kserve](https://github.com/kserve/community?tab=readme-ov-file#questions-and-issues))
- Attend our community meeting by subscribing to the [KServe calendar](https://zoom-lfx.platform.linuxfoundation.org/meetings/kserve?view=month).
- View our [community github repository](https://github.com/kserve/community) to learn how to make contributions. We are excited to work with you to make KServe better and promote its adoption!

---

*The KServe team is committed to making machine learning model serving simple, scalable, and standardized. Thank you for being part of our community!*
