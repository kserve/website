---
apiVersion: "serving.kserve.io/v1beta1"
kind: InferenceService
metadata:
  annotations:
    # The autoscaling target defines how the service should be auto-scale in
    # response to incoming requests. The value of 5 indicates that
    # additional containers should be deployed when the number of concurrent
    # requests exceeds 5. See KServe documentation for more information about autoscaling
    autoscaling.knative.dev/target: "5"
  labels:
    controller-tools.k8s.io: "1.0"
    app: example-amdserver-runtime-isvc
  name: example-amdserver-runtime-isvc
spec:
  predictor:
    model:
      modelFormat:
        name: tensorflow
      storageUri: https://github.com/Xilinx/inference-server/blob/main/tests/assets/mnist.zip
      # while it's optional for KServe, the runtime should be explicitly
      # specified to make sure the runtime you've added for the AMD Inference
      # Server is used
      runtime: kserve-amdserver
