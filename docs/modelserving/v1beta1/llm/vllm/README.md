## vLLM Runtime

The official vLLM support is available through [Hugging Face Serving Runtime](https://kserve.github.io/website/master/modelserving/v1beta1/llm/huggingface/).
