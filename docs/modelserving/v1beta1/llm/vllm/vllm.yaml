apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama-2-7b
spec:
  predictor:
    containers:
      - args:
          - --port
          - "8080"
          - --model
          - /mnt/models
        command:
          - python3
          - -m
          - vllm.entrypoints.api_server
        env:
          - name: STORAGE_URI
            value: gs://kfserving-examples/models/huggingface/llama # Upload the llama model on your cloud storage
        image: kserve/vllmserver:latest
        name: kserve-container
        resources:
          limits:
            cpu: "4"
            memory: 50Gi
            nvidia.com/gpu: "1"
          requests:
            cpu: "1"
            memory: 50Gi
            nvidia.com/gpu: "1"
    maxReplicas: 1
    minReplicas: 1

