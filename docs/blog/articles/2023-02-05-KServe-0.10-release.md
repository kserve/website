# Announcing: KServe v0.10.0

We are excited to announce KServe 0.10 release, in this release we have been focusing on enabling more KServe networking setup options,
improving metrics for supported serving runtimes and increasing support coverage for [Open(aka v2) inference protocol](https://kserve.github.io/website/0.10/modelserving/data_plane/v2_protocol/) for both KServe and ModelMesh.

## KServe Networking Options

Istio is now optional for both `Serverless` and `RawDeployment` mode, please see the [alternative networking guide](https://kserve.github.io/website/0.10/admin/serverless/kourier_networking/) to see how you can enable other ingress options supported by Knative with Serverless mode.
For Istio users, if you want to turn on full service mesh mode to secure inference services with mutual TLS and enable the traffic policies, please read the [service mesh setup guideline](https://kserve.github.io/website/0.10/admin/serverless/servicemesh/).

## KServe Telemetry for Serving Runtimes

Prometheus metrics are now available for supported serving runtimes including custom python runtimes,
in Serverless mode we have extended Knative queue-proxy to enable metrics aggregation for both metrics exposed in `queue-proxy` and `kserve-container`. 
Please read the [prometheus metrics setup guideline](https://kserve.github.io/website/0.10/modelserving/observability/prometheus_metrics/) for how to enable the metrics scraping and aggregations.

## Open(v2) Inference Protocol Support Coverage

In KServe 0.10, we have added support for Open(v2) inference protocol for KServe custom runtimes, 
now you can enable v2 REST/gRPC for both custom transformer and predictor with images built by implementing KServe Python SDK API.
gRPC enables high performance inference data plane as it is built on top of HTTP/2 and binary data transportation which is more efficient to send over the wire compared to REST.
Please see the detailed example for [transformer](https://kserve.github.io/website/0.10/modelserving/v1beta1/transformer/torchserve_image_transformer/) and 
[predictor](https://kserve.github.io/website/0.10/modelserving/v1beta1/custom/custom_model/).

```python
from kserve import Model

def image_transform(byte_array):
    image_processing = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    image = Image.open(io.BytesIO(byte_array))
    tensor = image_processing(image).numpy()
    return tensor

class CustomModel(Model):
    def predict(self, request: InferRequest, headers: Dict[str, str]) -> InferResponse:
        input_tensors = [image_transform(instance) for instance in request.inputs[0].data]
        input_tensors = np.asarray(input_tensors)
        output = self.model(input_tensors)
        torch.nn.functional.softmax(output, dim=1)
        values, top_5 = torch.topk(output, 5)
        result = values.flatten().tolist()
        response_id = generate_uuid()
        infer_output = InferOutput(name="output-0", shape=list(values.shape), datatype="FP32", data=result)
        infer_response = InferResponse(model_name=self.name, infer_outputs=[infer_output], response_id=response_id)
        return infer_response

class CustomTransformer(Model):
    def preprocess(self, request: InferRequest, headers: Dict[str, str]) -> InferRequest:
        input_tensors = [image_transform(instance) for instance in request.inputs[0].data]
        input_tensors = np.asarray(input_tensors)
        infer_inputs = [InferInput(name="INPUT__0", datatype='FP32', shape=list(input_tensors.shape),
                                   data=input_tensors)]
        infer_request = InferRequest(model_name=self.model_name, infer_inputs=infer_inputs)
        return infer_request
```

You can use the same Python API type `InferRequest` and `InferResponse` for both REST and gRPC protocol, KServe handles the underlying decoding and encoding according to the protocol.
New `headers` argument is added to the custom handlers to pass http/gRPC headers or metadata, you can also use this as context dict to pass data between handlers.


Please check the following table for complete support matrix for ServingRuntimes and ModelFormats.

| Model Format        | v1           | v2 REST/gRPC | 
| ------------------- |--------------| ----------------|
| Tensorflow          | ✅ TFServing    | ✅ Triton |
| PyTorch             | ✅ TorchServe   | ✅ TorchServe |
| TorchScript         | ✅ TorchServe   | ✅ Triton |
| ONNX                | ❌              | ✅ Triton |
| Scikit-learn        | ✅ KServe       | ✅ MLServer |
| XGBoost             | ✅ KServe       | ✅ MLServer |
| LightGBM            | ✅ KServe       | ✅ MLServer |
| MLFLow              | ❌              | ✅ MLServer |
| Custom              | ✅ KServe       | ✅ KServe |


## Multi-Arch Image Support

KServe control plane images [kserve-controller](https://hub.docker.com/r/kserve/kserve-controller/tags),
[kserve/agent](https://hub.docker.com/r/kserve/agent/tags), [kserve/router](https://hub.docker.com/r/kserve/router/tags) are now supported 
for multiple architectures: `ppc64le`, `arm64`, `amd64`, `s390x`.

## KServe Storage Credentials

- Currently, AWS users need to create a secret with long term/static IAM credentials for downloading models stored in S3.
  Security best practice is to use [IAM role for service account(IRSA)](https://aws.amazon.com/blogs/opensource/introducing-fine-grained-iam-roles-service-accounts/) 
  which enables automatic credential rotation and fine-grained access control, see how to [setup IRSA](https://kserve.github.io/website/0.10/modelserving/storage/s3/s3/#create-service-account-with-iam-role).
- Support Azure Blobs with [managed identity](https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/how-manage-user-assigned-managed-identities?pivots=identity-mi-methods-azcli).

## ModelMesh updates

- Support for TorchServe ServingRuntime.
- Support v2 inference protocol for OpenVINO ServingRuntime.
- `ClusterServingRuntime` support for ModelMesh which now works in the same way as KServe.
- Support passing labels and annotations from `ServingRuntimePodSpec` to ModelMesh Pods.
- Support for `ImagePullSecrets` on ServingRuntime spec.


## Other Changes:

For a complete change list please read the release notes from [KServe v0.10](https://github.com/kserve/kserve/releases/tag/v0.10.0) and
[ModelMesh v0.10](https://github.com/kserve/modelmesh-serving/releases/tag/v0.10.0).

## Join the community

- Visit our [Website](https://kserve.github.io/website/) or [GitHub](https://github.com/kserve)
- Join the Slack ([#kserve](https://kubeflow.slack.com/join/shared_invite/zt-n73pfj05-l206djXlXk5qdQKs4o1Zkg#/))
- Attend our community meeting by subscribing to the [KServe calendar](https://wiki.lfaidata.foundation/display/kserve/calendars).
- View our [community github repository](https://github.com/kserve/community) to learn how to make contributions. We are excited to work with you to make KServe better and promote its adoption!

Thank you for contributing or checking out KServe!

- The KServe Working Group
