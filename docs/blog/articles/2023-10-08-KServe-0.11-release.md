# Announcing: KServe v0.10.0

We are excited to announce the release of KServe 0.11, in this release we made enhancements to the KServe control plane and Python SDK. Additionally, we have introduced LLM runtimes and in ModelMesh we have added feature parities like PVC, HPA, payload logging support.
Here is a summary of the key changes:

### KServe Core Inference Enhancements

- Introduce Path based routing, more information can be found from [the issue](https://github.com/kserve/kserve/issues/2257) and [configuration docs](https://github.com/kserve/kserve/blob/294a10495b6b5cda9c64d3e1573b60aec62aceb9/config/configmap/inferenceservice.yaml#L237).

- Introduce priority field for Serving Runtime custom resource, see more details from [the doc](https://kserve.github.io/website/0.11/modelserving/servingruntimes/#priority).

- Introduce Custom Storage Container CRD to allow integrations like private model registry.

- Inference Graph enhancements for error response handling, for more details please refer to the PR.

- Improve InferenceService debugging experience by adding the aggregate `RoutesReady` status and `LastDeploymentReady` condition to the InferenceService Status to differentiate the endpoint and deployment status, 
  for more details refer to the [API docs](https://pkg.go.dev/github.com/kserve/kserve@v0.11.1/pkg/apis/serving/v1beta1#InferenceServiceStatus).

### Enhanced Python SDK Dependency Management

- We have adopted [poetry](https://python-poetry.org/docs/) to manage python dependencies. You can now install the KServe SDK with locked dependencies using `poetry install`. 
While `pip install` still works,  we highly recommend using poetry to ensure predictable dependency management.

- The KServe SDK is also slimmed down by making the cloud storage dependency optional, if you require storage dependency for custom serving runtimes you can still install with `pip install kserve[storage]`.


### KServe Python Runtimes Improvements
- KServe Python Runtimes including sklearnserver, lgbserver, xgbserver,  now support the open inference protocol for both REST and gRPC.

- Logging improvements including adding Uvicorn access logging and a default KServe logger.

- Postprocess handler has been aligned with open inference protocol, simplifying the underlying transportation protocol complexities.


### TorchServe LLM

KServe now integrates with TorchServe 0.8, offering the support for [LLM models](https://pytorch.org/serve/large_model_inference.html) that may not fit onto a single GPU. 
Huggingface Accelerate and Deepspeed are available options to split the model into multiple partitions over multiple GPUs. You can see the detailed examples for how to serve the LLM on KServe with TorchServe runtime.

## ModelMesh Updates

### Storing Models on Kubernetes Persistent Volumes (PVC)
ModelMesh now allows to [directly mount model files onto serving runtimes pods](https://github.com/kserve/modelmesh-serving/blob/main/docs/predictors/setup-storage.md#deploy-a-model-stored-on-a-persistent-volume-claim) 
using [Kubernetes Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/). Depending on the selected [storage solution](https://kubernetes.io/docs/concepts/storage/storage-classes/) this approach can significantly reduce latency when deploying new predictors, 
potentially remove the need for additional S3 cloud object storage like AWS S3, GCS, or Azure Blob Storage altogether.


### Horizontal Pod Autoscaling (HPA)
Kubernetes [Horizontal Pod Autoscaling](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) can now be used at the serving runtime pod level. With HPA enabled, the ModelMesh controller no longer manages the number of replicas. Instead, a `HorizontalPodAutoscaler` automatically updates the serving
runtime deployment with the number of Pods to best match the demand.

### Model Metrics, Metrics Dashboard, Payload Event Logging
ModelMesh v0.11 introduces a new configuration option to emit a subset of useful metrics at the individual model level. These metrics can help identify outlier or "heavy hitter" models and consequently fine-tune the deployments of those inference services, like allocating more resources or increasing the number of replicas for improved responsiveness or avoid frequent cache misses.

A new [Grafana dashboard](https://github.com/kserve/modelmesh-serving/blob/main/docs/monitoring.md#import-the-grafana-dashboard) was added to display the comprehensive set of [Prometheus metrics](https://github.com/kserve/modelmesh-serving/blob/main/docs/monitoring.md) like model loading
and unloading rates, internal queuing delays, capacity and usage, cache state, etc. to monitor the general health of the ModelMesh Serving deployment.

The new [`PayloadProcessor` interface](https://github.com/kserve/modelmesh/blob/main/src/main/java/com/ibm/watson/modelmesh/payload/) can be implemented to log prediction requests and responses, to create data sinks for data visualization, for model quality assessment, or for drift and outlier detection by external monitoring systems.

## What's Changed:
- The `Default` suffix in the inference service component(predictor/transformer/explainer) name has been removed. This affects the client that is using the component url directly.

- Status.address.url is now consistent for both serverless and raw deployment mode, the url path portion is dropped in serverless mode.

- Raw bytes are now accepted in v1 protocol, setting the content-type header to “application/json” is required to recognize and decode json payload.



For a complete change list please read the release notes from [KServe v0.11](https://github.com/kserve/kserve/releases/tag/v0.11.0) and
[ModelMesh v0.11](https://github.com/kserve/modelmesh-serving/releases/tag/v0.11.0).

## Join the community

- Visit our [Website](https://kserve.github.io/website/) or [GitHub](https://github.com/kserve)
- Join the Slack ([#kserve](https://kubeflow.slack.com/?redir=%2Farchives%2FCH6E58LNP))
- Attend our community meeting by subscribing to the [KServe calendar](https://wiki.lfaidata.foundation/display/kserve/calendars).
- View our [community github repository](https://github.com/kserve/community) to learn how to make contributions. We are excited to work with you to make KServe better and promote its adoption!


Thanks for all the contributors who have made the commits to 0.11 release!

The KServe Working Group
